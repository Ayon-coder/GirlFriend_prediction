# ğŸ§  Girlfriend Prediction Using Logistic Regression

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![NumPy](https://img.shields.io/badge/NumPy-1.21+-orange.svg)](https://numpy.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

A **learning project** focused on understanding core machine learning concepts by implementing Logistic Regression both from scratch and using scikit-learn.

> ğŸ“ **Purpose**: This project is designed to learn and demonstrate understanding of:
> - **Gradient descent optimization** from first principles
> - **Binary cross-entropy loss** function
> - **Sklearn Pipelines** for clean ML workflows
>
> *Uses synthetic data to focus on algorithm implementation rather than real-world prediction.*

> âš ï¸ **Disclaimer**: This is purely an educational project. Relationship outcomes depend on countless factors beyond what any model can capture!

---

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Notebooks](#notebooks)
- [Dataset](#dataset)
- [Technical Approach](#technical-approach)
- [Results](#results)
- [Installation](#installation)
- [Usage](#usage)
- [Project Structure](#project-structure)
- [Learning Outcomes](#learning-outcomes)

---

## ğŸ¯ Overview

This project contains **two implementations** of Logistic Regression:

| Notebook | Approach | Purpose |
|----------|----------|----------|
| `gf_pred_manual.ipynb` | From scratch with NumPy | Understand gradient descent |
| `gf_pred_sklearn.ipynb` | Using sklearn Pipeline | Learn production-style workflow |

### Key Highlights
- âœ¨ Manual implementation with no ML libraries
- ğŸ“Š Custom gradient descent with 100,000 iterations
- ğŸ“ˆ Visualization of cost function convergence
- ğŸ”§ Sklearn Pipeline for comparison

---

## ğŸ“Š Dataset

The dataset (`indian_boys_gf_prediction_balanced.csv`) contains **300 balanced samples** with the following features:

| Feature | Description | Range |
|---------|-------------|-------|
| `age` | Age in years | 18-30 |
| `height_cm` | Height in centimeters | 155-190 |
| `income_lpa` | Annual income (Lakhs/year) | 1.5-20 |
| `fitness_level` | Self-rated fitness score | 1-10 |
| `confidence` | Self-rated confidence score | 1-10 |
| `social_media_hours` | Daily social media usage (hours) | 0.5-6.0 |
| **`has_gf`** | Target variable | 0 (No) / 1 (Yes) |

### Dataset Distribution
- **Class 0 (No GF)**: ~50%
- **Class 1 (Has GF)**: ~50%

---

## ğŸ”¬ Technical Approach

### 1. Feature Scaling (Standardization)
```
z = (x - Î¼) / Ïƒ
```
Ensures all features are on the same scale for faster gradient descent convergence.

### 2. Sigmoid Activation
```
Ïƒ(z) = 1 / (1 + e^(-z))
```
Maps linear output to probability between 0 and 1.

### 3. Binary Cross-Entropy Loss
```
J(W, B) = -(1/m) Î£ [yÂ·log(Å·) + (1-y)Â·log(1-Å·)]
```

### 4. Gradient Descent Update
```
W = W - Î± Â· (âˆ‚J/âˆ‚W)
B = B - Î± Â· (âˆ‚J/âˆ‚B)
```
Where Î± = 0.1 (learning rate)

---

## ğŸ“ˆ Results

| Metric | Value |
|--------|-------|
| Training Accuracy | ~65% |
| Test Accuracy | ~54% |
| Iterations | 100,000 |
| Learning Rate | 0.1 |

### Observations
- Model converges successfully (cost function decreases)
- Moderate accuracy expected due to:
  - Inherently noisy/random nature of the target
  - Limited feature set
  - Small dataset size

---

## ğŸš€ Installation

### Prerequisites
- Python 3.8+
- pip

### Setup
```bash
# Clone the repository
git clone https://github.com/yourusername/logistic_regression_gf_pred_proj.git
cd logistic_regression_gf_pred_proj

# Install dependencies
pip install pandas numpy matplotlib scikit-learn jupyter
```

---

## ğŸ’» Usage

### Run the Notebooks
```bash
# Manual implementation (from scratch)
jupyter notebook gf_pred_manual.ipynb

# Sklearn implementation
jupyter notebook gf_pred_sklearn.ipynb
```

### Make a Prediction
```python
# Example: Predict for a new user
user_input = pd.DataFrame([{
    "age": 25,
    "height_cm": 180,
    "income_lpa": 12,
    "fitness_level": 10,
    "confidence": 7,
    "social_media_hours": 5
}])

# Scale using training statistics
scaled_input = (user_input.values - X_train.mean(axis=0)) / X_train.std(axis=0)

# Get probability
probability = sigmoid(np.dot(scaled_input, W) + B)
print(f"Probability of having GF: {probability[0][0]:.2%}")
```

---

## ğŸ“ Project Structure

```
logistic_regression_gf_pred_proj/
â”œâ”€â”€ gf_pred_manual.ipynb                    # From-scratch implementation
â”œâ”€â”€ gf_pred_sklearn.ipynb                   # Sklearn Pipeline implementation
â”œâ”€â”€ indian_boys_gf_prediction_balanced.csv  # Dataset
â”œâ”€â”€ add_docs.py                             # Docs script (manual notebook)
â”œâ”€â”€ add_docs_sklearn.py                     # Docs script (sklearn notebook)
â””â”€â”€ README.md                               # This file
```

---

## ğŸ“ Learning Outcomes

This project teaches:
- âœ… Logistic Regression theory and math
- âœ… Gradient Descent from first principles
- âœ… Binary Cross-Entropy loss function
- âœ… Feature scaling importance
- âœ… Train/Test split to prevent overfitting
- âœ… Sklearn Pipelines for clean workflows

---

## ğŸ”® Next Steps

For future projects with real-world data:
1. **EDA**: Exploratory Data Analysis before modeling
2. **Feature Engineering**: Create meaningful features
3. **Cross-Validation**: k-fold CV for robust evaluation
4. **Hyperparameter Tuning**: GridSearchCV / RandomizedSearchCV
5. **Model Comparison**: Try multiple algorithms

---

## ğŸ“š Learning Outcomes

This project teaches:
- âœ… Logistic Regression theory and implementation
- âœ… Gradient Descent optimization
- âœ… Binary Cross-Entropy loss function
- âœ… Feature scaling importance
- âœ… Train/Test split to prevent overfitting
- âœ… Model evaluation metrics

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ¤ Contributing

Contributions are welcome! Feel free to:
1. Fork the repository
2. Create a feature branch
3. Submit a pull request

---

<p align="center">
  Made with â¤ï¸ for learning Machine Learning
</p>
